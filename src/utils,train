kaggle datasets download -d khiemdungdang/mnist
unzip mnist.zip -d data/
src/utils.py
# src/utils.py
import numpy as np
import os
import pickle
from typing import Tuple

RNG = np.random.RandomState

def set_seed(seed: int = 42):
    np.random.seed(seed)

def one_hot(y, num_classes=None):
    y = np.array(y, dtype=int)
    if num_classes is None:
        num_classes = np.max(y) + 1
    oh = np.zeros((len(y), num_classes))
    oh[np.arange(len(y)), y] = 1
    return oh

def batch_iter(X, y, batch_size, shuffle=True, rng=None):
    n = X.shape[0]
    indices = np.arange(n)
    if shuffle:
        if rng is None:
            rng = np.random
        rng.shuffle(indices)
    for start in range(0, n, batch_size):
        excerpt = indices[start:start + batch_size]
        yield X[excerpt], y[excerpt]

def accuracy(y_true, y_pred):
    return np.mean(y_true == y_pred)

def save_results(path, obj):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'wb') as f:
        pickle.dump(obj, f)
2-src/train.py
# src/train.py
import numpy as np
import argparse
import os
from src.utils import set_seed, one_hot, batch_iter, accuracy, save_results

class LogisticSoftmax:
    def __init__(self, n_features, n_classes, weight_scale=0.01):
        self.W = weight_scale * np.random.randn(n_features, n_classes)
        self.b = np.zeros((1, n_classes))

    def predict_proba(self, X):
        logits = X.dot(self.W) + self.b
        logits = logits - np.max(logits, axis=1, keepdims=True)
        exp = np.exp(logits)
        return exp / np.sum(exp, axis=1, keepdims=True)

    def predict(self, X):
        return np.argmax(self.predict_proba(X), axis=1)

    def loss_and_grad(self, X, y_onehot, reg=0.0):
        n = X.shape[0]
        probs = self.predict_proba(X)
        loss = -np.sum(y_onehot * np.log(probs + 1e-12)) / n
        loss += 0.5 * reg * np.sum(self.W * self.W)
        dlogits = (probs - y_onehot) / n
        dW = X.T.dot(dlogits) + reg * self.W
        db = np.sum(dlogits, axis=0, keepdims=True)
        return loss, dW, db


def train(X_train, y_train, X_val, y_val,
          optimizer='gd', lr=0.1, epochs=50, batch_size=64,
          reg=0.0, seed=42, verbose=True):
    set_seed(seed)
    n_features = X_train.shape[1]
    n_classes = np.max(y_train) + 1
    model = LogisticSoftmax(n_features, n_classes)

    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}

    rng = np.random.RandomState(seed)

    for epoch in range(epochs):
        if optimizer == 'gd':
            loss, dW, db = model.loss_and_grad(X_train, one_hot(y_train, n_classes), reg=reg)
            model.W -= lr * dW
            model.b -= lr * db
        elif optimizer == 'sgd':
            # iterate samples
            for i in range(X_train.shape[0]):
                Xi = X_train[i:i+1]
                yi = one_hot(y_train[i:i+1], n_classes)
                loss_i, dW_i, db_i = model.loss_and_grad(Xi, yi, reg=reg)
                model.W -= lr * dW_i
                model.b -= lr * db_i
        elif optimizer == 'mb':
            for Xb, yb in batch_iter(X_train, y_train, batch_size, shuffle=True, rng=rng):
                loss_b, dW_b, db_b = model.loss_and_grad(Xb, one_hot(yb, n_classes), reg=reg)
                model.W -= lr * dW_b
                model.b -= lr * db_b
        else:
            raise ValueError('Unknown optimizer')

        train_loss, _, _ = model.loss_and_grad(X_train, one_hot(y_train, n_classes), reg=reg)
        val_loss, _, _ = model.loss_and_grad(X_val, one_hot(y_val, n_classes), reg=reg)
        train_acc = accuracy(y_train, model.predict(X_train))
        val_acc = accuracy(y_val, model.predict(X_val))

        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)

        if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):
            print(f"Epoch {epoch+1}/{epochs} | train_loss={train_loss:.4f} val_loss={val_loss:.4f} | train_acc={train_acc:.4f} val_acc={val_acc:.4f}")

    return model, history


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--optimizer', choices=['gd','sgd','mb'], default='mb')
    parser.add_argument('--lr', type=float, default=0.1)
    parser.add_argument('--epochs', type=int, default=50)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--reg', type=float, default=0.0)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--data_dir', type=str, default='data/')
    parser.add_argument('--dataset', type=str, default='mnist', help='mnist or custom')
    args = parser.parse_args()

    # minimal data loader: expects preprocessed numpy arrays in data/
    # You should replace this with proper Kaggle dataset loading.
    import numpy as _np
    def load_simple_mnist(d):
        # expect: data/X_train.npy, data/y_train.npy, data/X_val.npy, data/y_val.npy
        X_train = _np.load(os.path.join(d, 'X_train.npy'))
        y_train = _np.load(os.path.join(d, 'y_train.npy'))
        X_val = _np.load(os.path.join(d, 'X_val.npy'))
        y_val = _np.load(os.path.join(d, 'y_val.npy'))
        return X_train, y_train, X_val, y_val

    X_train, y_train, X_val, y_val = load_simple_mnist(args.data_dir)

    model, history = train(X_train, y_train, X_val, y_val,
                           optimizer=args.optimizer, lr=args.lr, epochs=args.epochs,
                           batch_size=args.batch_size, reg=args.reg, seed=args.seed)

    save_results(os.path.join('results', f'history_{args.optimizer}_lr{args.lr}.pkl'), history)
